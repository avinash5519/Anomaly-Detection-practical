{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "XTrain = []\n",
    "YTrain = []\n",
    "XTest = []\n",
    "YTest = []\n",
    "# trainPath = r'/UCI HAR Dataset/UCI HAR Dataset/train/'\n",
    "# testPath = r'/UCI HAR Dataset/UCI HAR Dataset/test/'\n",
    "trainPath = r'/Dataset/UCI HAR Dataset/UCI HAR Dataset/train/'\n",
    "testPath = r'/Dataset/UCI HAR Dataset/UCI HAR Dataset/test/'\n",
    "\n",
    "#XTrain = np.loadtxt(os.getcwd() + r'/UCI HAR Dataset/UCI HAR Dataset/train/X_train.txt', delimiter=\" \", dtype='str')\n",
    "xTrainFileName = os.getcwd() + trainPath + r'X_train.txt'\n",
    "with open(xTrainFileName, \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        XTrain.append(map(float, line.split()))\n",
    "        \n",
    "              \n",
    "yTrainFileName = os.getcwd() + trainPath + r'y_train.txt'\n",
    "with open(yTrainFileName, \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        YTrain.append(int(line.strip()))\n",
    "\n",
    "XTrain = np.asarray(XTrain)\n",
    "YTrain = np.asarray(YTrain)\n",
    "\n",
    "xTestFileName = os.getcwd() + testPath + r'X_test.txt'\n",
    "with open(xTestFileName, \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        XTest.append(map(float, line.split()))\n",
    "        \n",
    "              \n",
    "yTestFileName = os.getcwd() + testPath + r'y_test.txt'\n",
    "with open(yTestFileName, \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        YTest.append(int(line.strip()))\n",
    "        \n",
    "        \n",
    "XTest = np.asarray(XTest)\n",
    "YTest = np.asarray(YTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the time series data using Fast Fourier Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft\n",
    "# The first 265 columns represent the acceleration for all the axes in the Time Series Domain\n",
    "xfTrain = fft(XTrain[:,0:265])\n",
    "xfTest = fft(XTest[:,0:265])\n",
    "\n",
    "# timeSeriesFFT = []\n",
    "# for row in xf:\n",
    "#     timeSeriesFFT.append(row)\n",
    "\n",
    "# print (np.array(timeSeriesFFT)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate the acceleration for all the axes into one feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XTrain = np.append(XTrain[:,265:], xfTrain , 1)\n",
    "XTest = np.append(XTest[:,265:], xfTest , 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def classifyRandomForestClassifier(XTrain, XTest, YTrain, YTest):\n",
    "    clf = RandomForestClassifier(n_estimators=10)\n",
    "    clf.fit(XTrain, YTrain)\n",
    "    YPred = clf.predict(XTest)\n",
    "    diff = YPred - YTest\n",
    "    score = diff[diff == 0].size\n",
    "    return (100.0 * score)/(YPred.size), YPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#K Nearest Neighbours Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def classifyKNNClassifier(XTrain, XTest, YTrain, YTest):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "    YPred = neigh.fit(XTrain, YTrain).predict(XTest)\n",
    "    diff = YPred - YTest\n",
    "    score = diff[diff == 0].size\n",
    "    return (100.0 * score)/(YPred.size), YPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "from sklearn import linear_model\n",
    "def classifyLinearRegressionClassifier(XTrain, XTest, YTrain, YTest):\n",
    "    LR = linear_model.LinearRegression()\n",
    "    YPred = LR.fit(XTrain, YTrain).predict(XTest)\n",
    "    diff = YPred - YTest\n",
    "    score = diff[diff == 0].size\n",
    "    return (100.0 * score)/(YPred.size), YPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bayesian Ridge\n",
    "from sklearn import linear_model\n",
    "def classifyBayesianRidgeClassifier(XTrain, XTest, YTrain, YTest):\n",
    "    clflml = linear_model.BayesianRidge()\n",
    "    YPred = clflml.fit(XTrain, YTrain).predict(XTest)\n",
    "    diff = YPred - YTest\n",
    "    score = diff[diff == 0].size\n",
    "    return (100.0 * score)/(YPred.size), YPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ARD Regression\n",
    "from sklearn import linear_model\n",
    "def classifyARDRegressionClassifier(XTrain, XTest, YTrain, YTest):\n",
    "    clflml = linear_model.ARDRegression(compute_score=True)\n",
    "    YPred = clflml.fit(XTrain, YTrain).predict(XTest)\n",
    "    diff = YPred - YTest\n",
    "    score = diff[diff == 0].size\n",
    "    return (100.0 * score)/(YPred.size), YPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn import linear_model\n",
    "def classifyLogisticRegressionClassifier(XTrain, XTest, YTrain, YTest):\n",
    "    LogReg = linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None)\n",
    "    YPred = LogReg.fit(XTrain, YTrain).predict(XTest)\n",
    "    diff = YPred - YTest\n",
    "    score = diff[diff == 0].size\n",
    "    return (100.0 * score)/(YPred.size), YPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "from sklearn import tree\n",
    "def classifyDecisionTreeClassifier(XTrain, XTest, YTrain, YTest):\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    YPred = clf.fit(XTrain, YTrain).predict(XTest)\n",
    "    diff = YPred - YTest\n",
    "    score = diff[diff == 0].size\n",
    "    return (100.0 * score)/(YPred.size), YPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gaussian Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "def classifyGaussianNaiveBayesClassifier(XTrain, XTest, YTrain, YTest):\n",
    "    gnb = GaussianNB()\n",
    "    YPred = gnb.fit(XTrain, YTrain).predict(XTest)\n",
    "    diff = YPred - YTest\n",
    "    score = diff[diff == 0].size\n",
    "    return (100.0 * score)/(YPred.size), YPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Bernoulli Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "def classifyBernoulliNaiveBayesClassifier(XTrain, XTest, YTrain, YTest):\n",
    "    bnb = BernoulliNB()\n",
    "    YPred = bnb.fit(XTrain, YTrain).predict(XTest)\n",
    "    diff = YPred - YTest\n",
    "    score = diff[diff == 0].size\n",
    "    return (100.0 * score)/(YPred.size), YPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Multinomial Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "def classifyMultinomialNaiveBayesClassifier(XTrain, XTest, YTrain, YTest):\n",
    "    mnb = MultinomialNB()\n",
    "    YPred = mnb.fit(XTrain, YTrain).predict(XTest)\n",
    "    diff = YPred - YTest\n",
    "    score = diff[diff == 0].size\n",
    "    return (100.0 * score)/(YPred.size), YPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#One Vs Rest SVM Classifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "def classifyOneVsRestClassifier(XTrain, XTest, YTrain, YTest):\n",
    "    YPred = OneVsRestClassifier(LinearSVC(random_state=0)).fit(XTrain, YTrain).predict(XTest)\n",
    "    diff = YPred - YTest\n",
    "    score = diff[diff == 0].size\n",
    "    return (100.0 * score)/(YPred.size), YPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#One Vs One SVM Classifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "def classifyOneVsOneClassifier(XTrain, XTest, YTrain, YTest):\n",
    "    YPred = OneVsOneClassifier(LinearSVC(random_state=0)).fit(XTrain, YTrain).predict(XTest)\n",
    "    diff = YPred - YTest\n",
    "    score = diff[diff == 0].size\n",
    "    return (100.0 * score)/(YPred.size), YPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Output Code SVM Classifier\n",
    "from sklearn.multiclass import OutputCodeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "def classifyOutputCodeClassifier(XTrain, XTest, YTrain, YTest):\n",
    "    clf = OutputCodeClassifier(LinearSVC(random_state=0),code_size=2, random_state=0)\n",
    "    YPred = clf.fit(XTrain, YTrain).predict(XTest)\n",
    "    diff = YPred - YTest\n",
    "    score = diff[diff == 0].size\n",
    "    return (100.0 * score)/(YPred.size), YPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Multi Class SVM\n",
    "from sklearn import svm\n",
    "from sklearn.svm import NuSVC\n",
    "def classifyMultiClassSVMClassifier(XTrain, XTest, YTrain, YTest):\n",
    "    YPred = svm.SVC(kernel='linear').fit(XTrain, YTrain).predict(XTest)\n",
    "    diff = YPred - YTest\n",
    "    score = diff[diff == 0].size\n",
    "    return (100.0 * score)/(YPred.size), YPred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the signals into different activities. <br> </br> Use Leave-One-Out crossvalidation, where you test on one user and train on the rest of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn import cross_validation\n",
    "# # X = xf\n",
    "# y = YTrain\n",
    "# N = 7352\n",
    "# loo = cross_validation.LeaveOneOut(N)\n",
    "# print loo\n",
    "\n",
    "# totalScore = 0.0\n",
    "# for train_index, test_index in loo:\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "#     score, YPred = classifyGaussianNaiveBayesClassifier(X_train, X_test, y_train, y_test)\n",
    "#     totalScore += score\n",
    "# totalScore = totalScore/N\n",
    "# print totalScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stratified K Fold Cross Validation\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "def stratifiedKFoldVal(XTrain, YTrain, classify):\n",
    "    n_folds = 10\n",
    "    totalScore = 0.0\n",
    "    skf = StratifiedKFold(YTrain, n_folds)\n",
    "    for train_index, test_index in skf:\n",
    "        X_train, X_test = XTrain[train_index], XTrain[test_index]\n",
    "        y_train, y_test = YTrain[train_index], YTrain[test_index]\n",
    "        score, YPred = classify(X_train, X_test,  y_train, y_test)\n",
    "        totalScore += score\n",
    "    return totalScore/n_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.8574050244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:373: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  array = np.array(array, dtype=dtype, order=order, copy=copy)\n"
     ]
    }
   ],
   "source": [
    "classify = classifyRandomForestClassifier\n",
    "score = stratifiedKFoldVal(XTrain, YTrain, classify)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.9656713006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/numpy/core/numeric.py:460: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "classify = classifyKNNClassifier\n",
    "score = stratifiedKFoldVal(XTrain, YTrain, classify)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# classify = classifyLinearRegressionClassifier\n",
    "# score = stratifiedKFoldVal(XTrain, YTrain, classify)\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# classify = classifyBayesianRidgeClassifier\n",
    "# score = stratifiedKFoldVal(XTrain, YTrain, classify)\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classify = classifyARDRegressionClassifier\n",
    "# score = stratifiedKFoldVal(XTrain, YTrain, classify)\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classify = classifyLogisticRegressionClassifier\n",
    "# score = stratifiedKFoldVal(XTrain, YTrain, classify)\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.9186961915\n"
     ]
    }
   ],
   "source": [
    "classify = classifyDecisionTreeClassifier\n",
    "score = stratifiedKFoldVal(XTrain, YTrain, classify)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.9947542962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/naive_bayes.py:378: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  self.theta_[i, :] = new_theta\n"
     ]
    }
   ],
   "source": [
    "classify = classifyGaussianNaiveBayesClassifier\n",
    "score = stratifiedKFoldVal(XTrain, YTrain, classify)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# classify = classifyMultinomialNaiveBayesClassifier\n",
    "# score = stratifiedKFoldVal(XTrain, YTrain, classify)\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/naive_bayes.py:758: DeprecationWarning: Implicitly casting between incompatible kinds. In a future numpy release, this will raise an error. Use casting=\"unsafe\" if this is intentional.\n",
      "  self.feature_count_ += safe_sparse_dot(Y.T, X)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/naive_bayes.py:758: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  self.feature_count_ += safe_sparse_dot(Y.T, X)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/naive_bayes.py:758: DeprecationWarning: Implicitly casting between incompatible kinds. In a future numpy release, this will raise an error. Use casting=\"unsafe\" if this is intentional.\n",
      "  self.feature_count_ += safe_sparse_dot(Y.T, X)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/naive_bayes.py:758: DeprecationWarning: Implicitly casting between incompatible kinds. In a future numpy release, this will raise an error. Use casting=\"unsafe\" if this is intentional.\n",
      "  self.feature_count_ += safe_sparse_dot(Y.T, X)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/naive_bayes.py:758: DeprecationWarning: Implicitly casting between incompatible kinds. In a future numpy release, this will raise an error. Use casting=\"unsafe\" if this is intentional.\n",
      "  self.feature_count_ += safe_sparse_dot(Y.T, X)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/naive_bayes.py:758: DeprecationWarning: Implicitly casting between incompatible kinds. In a future numpy release, this will raise an error. Use casting=\"unsafe\" if this is intentional.\n",
      "  self.feature_count_ += safe_sparse_dot(Y.T, X)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/naive_bayes.py:758: DeprecationWarning: Implicitly casting between incompatible kinds. In a future numpy release, this will raise an error. Use casting=\"unsafe\" if this is intentional.\n",
      "  self.feature_count_ += safe_sparse_dot(Y.T, X)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/naive_bayes.py:758: DeprecationWarning: Implicitly casting between incompatible kinds. In a future numpy release, this will raise an error. Use casting=\"unsafe\" if this is intentional.\n",
      "  self.feature_count_ += safe_sparse_dot(Y.T, X)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/naive_bayes.py:758: DeprecationWarning: Implicitly casting between incompatible kinds. In a future numpy release, this will raise an error. Use casting=\"unsafe\" if this is intentional.\n",
      "  self.feature_count_ += safe_sparse_dot(Y.T, X)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/naive_bayes.py:758: DeprecationWarning: Implicitly casting between incompatible kinds. In a future numpy release, this will raise an error. Use casting=\"unsafe\" if this is intentional.\n",
      "  self.feature_count_ += safe_sparse_dot(Y.T, X)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/naive_bayes.py:758: DeprecationWarning: Implicitly casting between incompatible kinds. In a future numpy release, this will raise an error. Use casting=\"unsafe\" if this is intentional.\n",
      "  self.feature_count_ += safe_sparse_dot(Y.T, X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.1802261018\n"
     ]
    }
   ],
   "source": [
    "classify = classifyBernoulliNaiveBayesClassifier\n",
    "score = stratifiedKFoldVal(XTrain, YTrain, classify)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# classify = classifyOneVsRestClassifier\n",
    "# score = stratifiedKFoldVal(XTrain, YTrain, classify)\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# classify = classifyOneVsOneClassifier\n",
    "# score = stratifiedKFoldVal(XTrain, YTrain, classify)\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.8525338659\n"
     ]
    }
   ],
   "source": [
    "classify = classifyOutputCodeClassifier\n",
    "score = stratifiedKFoldVal(XTrain, YTrain, classify)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.6985423391\n"
     ]
    }
   ],
   "source": [
    "classify = classifyMultiClassSVMClassifier\n",
    "score = stratifiedKFoldVal(XTrain, YTrain, classify)\n",
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a Python framework for mixture of experts, \n",
    "### where the user can assign multiple classifiers to a different region of the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "np.random.seed(9)\n",
    "X_train, X_test, y_train, y_test = train_test_split(XTrain, YTrain, test_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def EnsembleFeatureSplitClassifiers(X_train, X_test, y_train, y_test, splitFeatIdx, classifiersList = [classifyKNNClassifier, classifyGaussianNaiveBayesClassifier, classifyOutputCodeClassifier, classifyMultiClassSVMClassifier]):\n",
    "    assert(len(classifiersList)*2 == len(splitFeatIdx))\n",
    "    \n",
    "#     s1,YPred1 = classifyRandomForestClassifier(X_train[:,splitFeatIdx[0]:splitFeatIdx[1]], X_test[:,splitFeatIdx[0]:splitFeatIdx[1]], y_train, y_test)\n",
    "#     s2,YPred2 = classifyKNNClassifier(X_train[:,splitFeatIdx[2]:splitFeatIdx[3]], X_test[:,splitFeatIdx[2]:splitFeatIdx[3]], y_train, y_test)\n",
    "#     s3,YPred3 = classifyGaussianNaiveBayesClassifier(X_train[:,splitFeatIdx[4]:splitFeatIdx[5]], X_test[:,splitFeatIdx[4]:splitFeatIdx[5]], y_train, y_test)\n",
    "#     s4,YPred4 = classifyMultiClassSVMClassifier(X_train[:,splitFeatIdx[6]:splitFeatIdx[7]], X_test[:,splitFeatIdx[6]:splitFeatIdx[7]], y_train, y_test)\n",
    "#     # Combine Y Predicted values from different classifiers\n",
    "#     XCombinedOp = np.column_stack((YPred1, YPred2, YPred3, YPred4))\n",
    "\n",
    "    YPred = []\n",
    "    i = 0\n",
    "    for classifier in classifiersList:\n",
    "        s,ypred = classifier(X_train[:,splitFeatIdx[i]:splitFeatIdx[i+1]], X_test[:,splitFeatIdx[i]:splitFeatIdx[i+1]], y_train, y_test)\n",
    "        i = i + 2\n",
    "        # Combine Y Predicted values from different classifiers\n",
    "        YPred.append(ypred)\n",
    "    \n",
    "    XCombinedOp = np.column_stack(YPred)\n",
    "    \n",
    "    return XCombinedOp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results of these classifiers are forwarded to an additional classifier \n",
    "### (e.g. neural network) that is trained to optimally combine the output from the first stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named sknn.mlp",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-00adb137a68f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msknn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmlp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mNeuralNetworkCombiner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXCombinedOpTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXCombinedOpTest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYTest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     nn = Classifier(layers=[Layer(\"Rectifier\", units=100),Layer(\"Linear\")],\n\u001b[0;32m      5\u001b[0m         \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.02\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named sknn.mlp"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from sknn.mlp import Classifier, Layer\n",
    "except ImportError:\n",
    "    print 'Please install scikit-neuralnetwork(pip install scikit-neuralnetwork)'\n",
    "\n",
    "def NeuralNetworkCombiner(XCombinedOpTrain, y_train, XCombinedOpTest, YTest):\n",
    "    nn = Classifier(layers=[Layer(\"Rectifier\", units=100),Layer(\"Linear\")],\n",
    "        learning_rate=0.02,\n",
    "        n_iter=10)\n",
    "    nn.fit(XCombinedOpTrain, y_train)\n",
    "    YTestPred = nn.predict(XCombinedOpTest)\n",
    "    diff = YTestPred - YTest.reshape(YTestPred.shape)\n",
    "    score = diff[diff == 0].size\n",
    "    score = (100.0 * score)/(YTestPred.size)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoreDisjointFeatures = []\n",
    "scoreOverlappingFeatures = []\n",
    "scoreAllFeatures = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using disjoint features for all classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With only 1 classifier\n",
    "classifiersList = [classifyKNNClassifier]\n",
    "featSplit = [0,561]\n",
    "\n",
    "XCombinedOpTrain = EnsembleFeatureSplitClassifiers(X_train, X_test, y_train, y_test, featSplit, classifiersList)\n",
    "XCombinedOpTest = EnsembleFeatureSplitClassifiers(X_train, XTest, y_train, YTest, featSplit, classifiersList)\n",
    "score1 = NeuralNetworkCombiner(XCombinedOpTrain, y_test, XCombinedOpTest, YTest)\n",
    "print score1\n",
    "scoreDisjointFeatures.append(score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With 2 classifiers\n",
    "featSplit = [0,230,230,561]\n",
    "classifiersList = [classifyKNNClassifier, classifyGaussianNaiveBayesClassifier]\n",
    "#classifiersList = [classifyGaussianNaiveBayesClassifier, classifyKNNClassifier]\n",
    "\n",
    "XCombinedOpTrain = EnsembleFeatureSplitClassifiers(X_train, X_test, y_train, y_test, featSplit, classifiersList)\n",
    "XCombinedOpTest = EnsembleFeatureSplitClassifiers(X_train, XTest, y_train, YTest, featSplit, classifiersList)\n",
    "score2 = NeuralNetworkCombiner(XCombinedOpTrain, y_test, XCombinedOpTest, YTest)\n",
    "print score2\n",
    "scoreDisjointFeatures.append(score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With 3 classifiers\n",
    "classifiersList = [classifyKNNClassifier, classifyGaussianNaiveBayesClassifier, classifyOutputCodeClassifier]\n",
    "featSplit = [0,185,185,370,370,561]\n",
    "\n",
    "XCombinedOpTrain = EnsembleFeatureSplitClassifiers(X_train, X_test, y_train, y_test, featSplit, classifiersList)\n",
    "XCombinedOpTest = EnsembleFeatureSplitClassifiers(X_train, XTest, y_train, YTest, featSplit, classifiersList)\n",
    "score3 = NeuralNetworkCombiner(XCombinedOpTrain, y_test, XCombinedOpTest, YTest)\n",
    "print score3\n",
    "scoreDisjointFeatures.append(score3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With 4 classifiers\n",
    "classifiersList = [classifyKNNClassifier, classifyGaussianNaiveBayesClassifier, classifyOutputCodeClassifier, classifyMultiClassSVMClassifier]\n",
    "featSplit = [0,140,140,280,280,420,420,561]\n",
    "\n",
    "XCombinedOpTrain = EnsembleFeatureSplitClassifiers(X_train, X_test, y_train, y_test, featSplit)\n",
    "XCombinedOpTest = EnsembleFeatureSplitClassifiers(X_train, XTest, y_train, YTest, featSplit)\n",
    "score4 = NeuralNetworkCombiner(XCombinedOpTrain, y_test, XCombinedOpTest, YTest)\n",
    "print score4\n",
    "scoreDisjointFeatures.append(score4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does the accuracy change if you use overlapping regions instead of disjunct ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With 1 classifier\n",
    "classifiersList = [classifyKNNClassifier]\n",
    "featSplit = [0,561]\n",
    "\n",
    "XCombinedOpTrain = EnsembleFeatureSplitClassifiers(X_train, X_test, y_train, y_test, featSplit, classifiersList)\n",
    "XCombinedOpTest = EnsembleFeatureSplitClassifiers(X_train, XTest, y_train, YTest, featSplit, classifiersList)\n",
    "score1 = NeuralNetworkCombiner(XCombinedOpTrain, y_test, XCombinedOpTest, YTest)\n",
    "print score1\n",
    "scoreOverlappingFeatures.append(score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With 2 classifiers\n",
    "featSplit = [0,290,220,561]\n",
    "classifiersList = [classifyKNNClassifier, classifyGaussianNaiveBayesClassifier]\n",
    "\n",
    "XCombinedOpTrain = EnsembleFeatureSplitClassifiers(X_train, X_test, y_train, y_test, featSplit, classifiersList)\n",
    "XCombinedOpTest = EnsembleFeatureSplitClassifiers(X_train, XTest, y_train, YTest, featSplit, classifiersList)\n",
    "score2 = NeuralNetworkCombiner(XCombinedOpTrain, y_test, XCombinedOpTest, YTest)\n",
    "print score2\n",
    "scoreOverlappingFeatures.append(score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With 3 classifiers\n",
    "classifiersList = [classifyKNNClassifier, classifyGaussianNaiveBayesClassifier, classifyOutputCodeClassifier]\n",
    "featSplit = [0,280,120,340,260,561]\n",
    "\n",
    "XCombinedOpTrain = EnsembleFeatureSplitClassifiers(X_train, X_test, y_train, y_test, featSplit, classifiersList)\n",
    "XCombinedOpTest = EnsembleFeatureSplitClassifiers(X_train, XTest, y_train, YTest, featSplit, classifiersList)\n",
    "score3 = NeuralNetworkCombiner(XCombinedOpTrain, y_test, XCombinedOpTest, YTest)\n",
    "print score3\n",
    "scoreOverlappingFeatures.append(score3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using overlapping features for all classifiers\n",
    "featSplit = [0,280,120,320,180,360,240,561]\n",
    "XCombinedOpTrain = EnsembleFeatureSplitClassifiers(X_train, X_test, y_train, y_test, featSplit)\n",
    "XCombinedOpTest = EnsembleFeatureSplitClassifiers(X_train, XTest, y_train, YTest, featSplit)\n",
    "score4 = NeuralNetworkCombiner(XCombinedOpTrain, y_test, XCombinedOpTest, YTest)\n",
    "print score4\n",
    "scoreOverlappingFeatures.append(score4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With All features being used in all of the Classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With 1 classifier\n",
    "classifiersList = [classifyKNNClassifier]\n",
    "featSplit = [0,561]\n",
    "\n",
    "XCombinedOpTrain = EnsembleFeatureSplitClassifiers(X_train, X_test, y_train, y_test, featSplit, classifiersList)\n",
    "XCombinedOpTest = EnsembleFeatureSplitClassifiers(X_train, XTest, y_train, YTest, featSplit, classifiersList)\n",
    "score1 = NeuralNetworkCombiner(XCombinedOpTrain, y_test, XCombinedOpTest, YTest)\n",
    "print score1\n",
    "scoreAllFeatures.append(score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With 2 classifiers\n",
    "featSplit = [0,561,0,561]\n",
    "classifiersList = [classifyKNNClassifier, classifyGaussianNaiveBayesClassifier]\n",
    "\n",
    "XCombinedOpTrain = EnsembleFeatureSplitClassifiers(X_train, X_test, y_train, y_test, featSplit, classifiersList)\n",
    "XCombinedOpTest = EnsembleFeatureSplitClassifiers(X_train, XTest, y_train, YTest, featSplit, classifiersList)\n",
    "score2 = NeuralNetworkCombiner(XCombinedOpTrain, y_test, XCombinedOpTest, YTest)\n",
    "print score2\n",
    "scoreAllFeatures.append(score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With 3 classifiers\n",
    "classifiersList = [classifyKNNClassifier, classifyGaussianNaiveBayesClassifier, classifyOutputCodeClassifier]\n",
    "featSplit = [0,561,0,561,0,561]\n",
    "\n",
    "XCombinedOpTrain = EnsembleFeatureSplitClassifiers(X_train, X_test, y_train, y_test, featSplit, classifiersList)\n",
    "XCombinedOpTest = EnsembleFeatureSplitClassifiers(X_train, XTest, y_train, YTest, featSplit, classifiersList)\n",
    "score3 = NeuralNetworkCombiner(XCombinedOpTrain, y_test, XCombinedOpTest, YTest)\n",
    "print score3\n",
    "scoreAllFeatures.append(score3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using all features for all classifiers\n",
    "featSplit = [0,561,0,561,0,561,0,561]\n",
    "XCombinedOpTrain = EnsembleFeatureSplitClassifiers(X_train, X_test, y_train, y_test, featSplit)\n",
    "XCombinedOpTest = EnsembleFeatureSplitClassifiers(X_train, XTest, y_train, YTest, featSplit)\n",
    "score1 = NeuralNetworkCombiner(XCombinedOpTrain, y_test, XCombinedOpTest, YTest)\n",
    "print score1\n",
    "scoreAllFeatures.append(score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.neural_network import BernoulliRBM\n",
    "# model = BernoulliRBM(n_components=2, n_iter=50)\n",
    "# model.fit(XCombinedOpTrain, y_test)\n",
    "# print model.intercept_hidden_\n",
    "# print model.intercept_visible_\n",
    "# print model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph of change of accuracy w.r.t. the number of used classifiers in the first stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plotting the results\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(scoreDisjointFeatures, label = \"Disjoint features\")\n",
    "plt.plot(scoreOverlappingFeatures, label = \"Overlapping features\")\n",
    "plt.plot(scoreAllFeatures, label = \"All features\")\n",
    "\n",
    "labels = ['kNN', 'Gaussian Naive Bayes & kNN', 'kNN, Gaussian Naive Bayes & MultiClassSVM', 'Output Code, kNN, Gaussian Naive Bayes & MultiClassSVM' ]\n",
    "# You can specify a rotation for the tick labels in degrees or with keywords.\n",
    "plt.xticks(np.arange(len(labels)), labels, rotation='vertical')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of classifers used in the ensemble')\n",
    "# Place a legend to the right of this smaller figure.\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there any benefit of using mixture of experts on this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Disjoint features gives comparatively poor results than overlapping features, but it again depends on what features are passed to each of these classifiers. There is variance in the results if we change the disjoint features being passed to each classifier or if we increase the number of classifiers.\n",
    "\n",
    "2. Overlapping features gives better results but depends on the number of classifiers and the features which were passed to it. Moreover, using Overlapping features over more number of classifiers gives consistent results.\n",
    "\n",
    "3. As shown in the above graph, if we pass all the features to each of the classifiers it gives better results and then the results drop as we increase the number of classifiers.\n",
    "\n",
    "Note: All these results depend on the type of classifiers used in the ensemble and the features passed to it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
