{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 Approaches in anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "mat = scipy.io.loadmat('oc_514.mat')\n",
    "train = mat['x']\n",
    "xtrain = train[0,0][0]\n",
    "\n",
    "# Commented split of train and test as using k-fold cross validation approach for outlier detection.\n",
    "X_train, X_test = train_test_split(\n",
    "   xtrain, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection using KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Using Gaussian Kernel Density Estimation\n",
    "def GaussianKDE(X_train, X_test):\n",
    "    #pdf = KernelDensity(bandwidth=0.25, kernel='linear')\n",
    "    pdf = KernelDensity(bandwidth=0.1, kernel='gaussian')\n",
    "    pdf.fit(X_train)\n",
    "    resTrain = []\n",
    "    resTest = []\n",
    "    \n",
    "    for data in X_train[:,:]:\n",
    "        resTrain.append(np.exp(pdf.score_samples(data))[0])\n",
    "    \n",
    "    for data in X_test[:,:]:\n",
    "        resTest.append(np.exp(pdf.score_samples(data))[0])\n",
    "    \n",
    "    nresTrain = np.array(resTrain)\n",
    "    nresTest = np.array(resTest)\n",
    "    \n",
    "    # Assumption: \"Normal data instances occur in high probability\n",
    "    # regions of a stochastic model, while anomalies occur in the low\n",
    "    # probability regions of the stochastic model.\"\n",
    "    return nresTrain[nresTrain <= 0.05].size, nresTest[nresTest <= 0.05].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 81.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cross Validation using K-Fold so that every data sample contributes in Training\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "X_train = xtrain\n",
    "#print X_train.shape\n",
    "n_folds = 5\n",
    "kf = KFold(420, n_folds)\n",
    "error_train = error_test = 0\n",
    "for train, test in kf:\n",
    "    #print train,test\n",
    "    n_error_train, n_error_test = GaussianKDE(X_train[train,:], X_train[test,:])\n",
    "    error_test += n_error_test\n",
    "    error_train += n_error_train\n",
    "    \n",
    "print float(error_train)/n_folds , float(error_test)/n_folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the train and test set in the ratio of 3:1 Trying with different train & test data sizes\n",
    "\n",
    "# n_error_train, n_error_test = GaussianKDE(X_train[:315,:], X_train[315:,:])\n",
    "\n",
    "\n",
    "# print n_error_train, n_error_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of KDE for Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using KDE we got 81 anomalies out of 420 samples. \n"
     ]
    }
   ],
   "source": [
    "print \"Using KDE we got\", (error_train + error_test)/n_folds , \"anomalies out of 420 samples. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em> Observation: </em>\n",
    "* This performs poorly as the number of anomalies is higher than the expected number of 183 as defined in the webpage of the dataset. \n",
    "* We tweaked the parameters of KDE and found Gaussian kernel to detect the anomalies better.\n",
    "\n",
    "<em> Conclusion: </em>\n",
    "* Kernel Density Estimation (KDE) can be performed in any number of dimensions, though in practice the curse of dimensionality causes its performance to degrade in high dimensions which is the case with our dataset as it has 278 features.\n",
    "\n",
    "<em>Doubt: </em>\n",
    "* We got zero anomalies from the training samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection using One Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#xtrain = train[0,0][0]\n",
    "def OutlierUsingOneClassSVM(X_train, X_test):\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.font_manager\n",
    "    from sklearn import svm\n",
    "    import scipy.io as sio\n",
    "\n",
    "    # fit the model\n",
    "    #clf = svm.OneClassSVM(nu=0.2, kernel=\"rbf\", gamma=0.1)\n",
    "    clf = svm.OneClassSVM(nu=0.435, kernel=\"linear\", gamma=0.1)\n",
    "    clf.fit(X_train)\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    n_error_train = y_pred_train[y_pred_train == -1].size\n",
    "    n_error_test = y_pred_test[y_pred_test == -1].size\n",
    "\n",
    "    return n_error_train, n_error_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(420, 278)\n",
      "146.8 37.0\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation using K-Fold so that every data sample contributes in Training\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "X_train = xtrain\n",
    "print X_train.shape\n",
    "n_folds = 5\n",
    "kf = KFold(420, n_folds)\n",
    "error_train = error_test = 0\n",
    "for train, test in kf:\n",
    "    #print train,test\n",
    "    n_error_train, n_error_test = OutlierUsingOneClassSVM(X_train[train,:], X_train[test,:])\n",
    "    #print error_train, error_test\n",
    "    #print n_error_train, n_error_test\n",
    "    error_train += n_error_train\n",
    "    error_test += n_error_test\n",
    "    \n",
    "print float(error_train)/n_folds, float(error_test)/n_folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 37\n"
     ]
    }
   ],
   "source": [
    "# Split the train and test set in the ratio of 4:1\n",
    "n_error_train, n_error_test = OutlierUsingOneClassSVM(X_train[:336,:], X_train[336:,:])\n",
    "\n",
    "print n_error_train, n_error_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of One Class SVM for Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using One Class SVM we got 184 anomalies out of 420 samples. \n"
     ]
    }
   ],
   "source": [
    "print \"Using One Class SVM we got\", (n_error_train + n_error_test) , \"anomalies out of 420 samples. \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em> Observation: </em>\n",
    "* This performs well after tuning the kernel to be linear and with appropriate parameters as the number of anomalies matches the expected number of 183 as defined in the webpage of the dataset.\n",
    "* We tweaked the parameters of One Class SVM and found Linear kernel to detect the anomalies better.\n",
    "* Here, we got anomalies in both the test and train data set.\n",
    "\n",
    "<em> Conclusion: </em>\n",
    "* One Class SVM gives useful results in higher dimensions, which is the case with our dataset as it has 278 features. Hence it identifies more anomalies as compared to KDE which was found to be poor for higher dimensional data.\n",
    "* It tries to fit the training data irrespective of whether it is an outlier or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Outlier Factor (LOF)\n",
    "Code adapted from https://github.com/damjankuznar/pylof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "# def euclideanDistance(point1, point2):\n",
    "#     # Computes the Euclidean distance between two objects or points. \n",
    "#     # compute RMSE (root mean squared error)\n",
    "#     difference = point1 - point2\n",
    "#     rmse = (sum(map(lambda x: x**2, difference)) / len(difference))**0.5\n",
    "#     return rmse\n",
    "\n",
    "def euclideanDistance(instance1, instance2):\n",
    "    \"\"\"Computes the distance between two instances. Instances should be tuples of equal length.\n",
    "    Returns: Euclidean distance\n",
    "    Signature: ((attr_1_1, attr_1_2, ...), (attr_2_1, attr_2_2, ...)) -> float\"\"\"\n",
    "    def detect_value_type(attribute):\n",
    "        \"\"\"Detects the value type (number or non-number).\n",
    "        Returns: (value type, value casted as detected type)\n",
    "        Signature: value -> (str or float type, str or float value)\"\"\"\n",
    "        from numbers import Number\n",
    "        attribute_type = None\n",
    "        if isinstance(attribute, Number):\n",
    "            attribute_type = float\n",
    "            attribute = float(attribute)\n",
    "        else:\n",
    "            attribute_type = str\n",
    "            attribute = str(attribute)\n",
    "        return attribute_type, attribute\n",
    "    # check if instances are of same length\n",
    "    if len(instance1) != len(instance2):\n",
    "        raise AttributeError(\"Instances have different number of arguments.\")\n",
    "    # init differences vector\n",
    "    differences = [0] * len(instance1)\n",
    "    # compute difference for each attribute and store it to differences vector\n",
    "    for i, (attr1, attr2) in enumerate(zip(instance1, instance2)):\n",
    "        type1, attr1 = detect_value_type(attr1)\n",
    "        type2, attr2 = detect_value_type(attr2)\n",
    "        # raise error is attributes are not of same data type.\n",
    "        if type1 != type2:\n",
    "            raise AttributeError(\"Instances have different data types.\")\n",
    "        if type1 is float:\n",
    "            # compute difference for float\n",
    "            differences[i] = attr1 - attr2\n",
    "        else:\n",
    "            # compute difference for string\n",
    "            if attr1 == attr2:\n",
    "                differences[i] = 0\n",
    "            else:\n",
    "                differences[i] = 1\n",
    "    # compute RMSE (root mean squared error)\n",
    "    rmse = (sum(map(lambda x: x**2, differences)) / len(differences))**0.5\n",
    "    return rmse\n",
    "\n",
    "class LOF:\n",
    "    \"\"\"Helper class for performing LOF computations and instances normalization.\"\"\"\n",
    "    def __init__(self, instances, normalize=True, distance_function=euclideanDistance):\n",
    "        self.instances = instances\n",
    "        self.normalize = normalize\n",
    "        self.distance_function = distance_function\n",
    "        if normalize:\n",
    "            self.normalize_instances()\n",
    "\n",
    "    def compute_instance_attribute_bounds(self):\n",
    "        min_values = [float(\"inf\")] * len(self.instances[0]) #n.ones(len(self.instances[0])) * n.inf \n",
    "        max_values = [float(\"-inf\")] * len(self.instances[0]) #n.ones(len(self.instances[0])) * -1 * n.inf\n",
    "        for instance in self.instances:\n",
    "            min_values = tuple(map(lambda x,y: min(x,y), min_values,instance)) #n.minimum(min_values, instance)\n",
    "            max_values = tuple(map(lambda x,y: max(x,y), max_values,instance)) #n.maximum(max_values, instance)\n",
    "        self.max_attribute_values = max_values\n",
    "        self.min_attribute_values = min_values\n",
    "            \n",
    "    def normalize_instances(self):\n",
    "        \"\"\"Normalizes the instances and stores the infromation for rescaling new instances.\"\"\"\n",
    "        if not hasattr(self, \"max_attribute_values\"):\n",
    "            self.compute_instance_attribute_bounds()\n",
    "        new_instances = []\n",
    "        for instance in self.instances:\n",
    "            new_instances.append(self.normalize_instance(instance)) # (instance - min_values) / (max_values - min_values)\n",
    "        self.instances = new_instances\n",
    "        \n",
    "    def normalize_instance(self, instance):\n",
    "        return tuple(map(lambda value,max,min: (value-min)/(max-min) if max-min > 0 else 0, \n",
    "                         instance, self.max_attribute_values, self.min_attribute_values))\n",
    "        \n",
    "    def local_outlier_factor(self, min_pts, instance):\n",
    "        \"\"\"The (local) outlier factor of instance captures the degree to which we call instance an outlier.\n",
    "        min_pts is a parameter that is specifying a minimum number of instances to consider for computing LOF value.\n",
    "        Returns: local outlier factor\n",
    "        Signature: (int, (attr1, attr2, ...), ((attr_1_1, ...),(attr_2_1, ...), ...)) -> float\"\"\"\n",
    "        if self.normalize:\n",
    "            instance = self.normalize_instance(instance)\n",
    "        return local_outlier_factor(min_pts, instance, self.instances, distance_function=self.distance_function)\n",
    "\n",
    "def k_distance(k, instance, instances, distance_function=euclideanDistance):\n",
    "    #TODO: implement caching\n",
    "    \"\"\"Computes the k-distance of instance as defined in paper. It also gatheres the set of k-distance neighbours.\n",
    "    Returns: (k-distance, k-distance neighbours)\n",
    "    Signature: (int, (attr1, attr2, ...), ((attr_1_1, ...),(attr_2_1, ...), ...)) -> (float, ((attr_j_1, ...),(attr_k_1, ...), ...))\"\"\"\n",
    "    distances = {}\n",
    "    for instance2 in instances:\n",
    "        distance_value = distance_function(instance, instance2)\n",
    "        if distance_value in distances:\n",
    "            distances[distance_value].append(instance2)\n",
    "        else:\n",
    "            distances[distance_value] = [instance2]\n",
    "    distances = sorted(distances.items())\n",
    "    neighbours = []\n",
    "    [neighbours.extend(n[1]) for n in distances[:k]]\n",
    "    return distances[k - 1][0], neighbours\n",
    "\n",
    "def reachability_distance(k, instance1, instance2, instances, distance_function=euclideanDistance):\n",
    "    \"\"\"The reachability distance of instance1 with respect to instance2.\n",
    "    Returns: reachability distance\n",
    "    Signature: (int, (attr_1_1, ...),(attr_2_1, ...)) -> float\"\"\"\n",
    "    (k_distance_value, neighbours) = k_distance(k, instance2, instances, distance_function=distance_function)\n",
    "    return max([k_distance_value, distance_function(instance1, instance2)])\n",
    "\n",
    "def local_reachability_density(min_pts, instance, instances, **kwargs):\n",
    "    \"\"\"Local reachability density of instance is the inverse of the average reachability \n",
    "    distance based on the min_pts-nearest neighbors of instance.\n",
    "    Returns: local reachability density\n",
    "    Signature: (int, (attr1, attr2, ...), ((attr_1_1, ...),(attr_2_1, ...), ...)) -> float\"\"\"\n",
    "    (k_distance_value, neighbours) = k_distance(min_pts, instance, instances, **kwargs)\n",
    "    reachability_distances_array = [0]*len(neighbours) #n.zeros(len(neighbours))\n",
    "    for i, neighbour in enumerate(neighbours):\n",
    "        reachability_distances_array[i] = reachability_distance(min_pts, instance, neighbour, instances, **kwargs) \n",
    "    return len(neighbours) / sum(reachability_distances_array)\n",
    "\n",
    "def local_outlier_factor(min_pts, instance, instances, **kwargs):\n",
    "    \"\"\"The (local) outlier factor of instance captures the degree to which we call instance an outlier.\n",
    "    min_pts is a parameter that is specifying a minimum number of instances to consider for computing LOF value.\n",
    "    Returns: local outlier factor\n",
    "    Signature: (int, (attr1, attr2, ...), ((attr_1_1, ...),(attr_2_1, ...), ...)) -> float\"\"\"\n",
    "    (k_distance_value, neighbours) = k_distance(min_pts, instance, instances, **kwargs)\n",
    "    instance_lrd = local_reachability_density(min_pts, instance, instances, **kwargs)\n",
    "    lrd_ratios_array = [0]* len(neighbours)\n",
    "    for i, neighbour in enumerate(neighbours):\n",
    "        instances_without_instance = set(instances)\n",
    "        instances_without_instance.discard(neighbour)\n",
    "        neighbour_lrd = local_reachability_density(min_pts, neighbour, instances_without_instance, **kwargs)\n",
    "        lrd_ratios_array[i] = neighbour_lrd / instance_lrd\n",
    "    return sum(lrd_ratios_array) / len(neighbours)\n",
    "\n",
    "def outliers(k, instances, **kwargs):\n",
    "    \"\"\"Simple procedure to identify outliers in the dataset.\"\"\"\n",
    "    instances_value_backup = instances\n",
    "    outliers = []\n",
    "    for i, instance in enumerate(instances_value_backup):\n",
    "        instances = list(instances_value_backup)\n",
    "        instances.remove(instance)\n",
    "        l = LOF(instances, **kwargs)\n",
    "        value = l.local_outlier_factor(k, instance)\n",
    "        if value > 1:\n",
    "            outliers.append({\"lof\": value, \"instance\": instance, \"index\": i})\n",
    "    outliers.sort(key=lambda o: o[\"lof\"], reverse=True)\n",
    "    return outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = X_train[:336,:]\n",
    "lof = LOF(train)\n",
    "values = []\n",
    "for instance in X_train[336:,:]:\n",
    "    value = lof.local_outlier_factor(5, instance)\n",
    "    values.append(value)\n",
    "#     print value, instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    }
   ],
   "source": [
    "# print values\n",
    "lofs = np.array(values)\n",
    "n_error_test = lofs[lofs>1.0].size\n",
    "print n_error_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "values_train = []\n",
    "for instance in X_train[:336,:]:\n",
    "    value = lof.local_outlier_factor(5, instance)\n",
    "    values_train.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271\n"
     ]
    }
   ],
   "source": [
    "# print values_train\n",
    "lofs = np.array(values_train)\n",
    "n_error_train=lofs[lofs>1.0].size\n",
    "print n_error_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of LOF for Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Local Outlier Factor we got 342 anomalies out of 420 samples. \n"
     ]
    }
   ],
   "source": [
    "print \"Using Local Outlier Factor we got\", (n_error_train + n_error_test) , \"anomalies out of 420 samples. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em> Observation: </em>\n",
    "* Local Outlier Factor performs poorly as the number of anomalies is higher than the expected number of 183 as defined in the webpage of the dataset. \n",
    "* It takes a lot of time(>1.5 hours) with five nearest neighbours on a test set of size 336 which is slow compared to both KDE & One-class SVM.\n",
    "\n",
    "<em> Conclusion: </em>\n",
    "* Local Outlier Factor (LOF) can be performed in any number of dimensions. In the ideal case, it should be better than the KDE and One Class SVM. One of the reasones for this could be that the value of LOF used to classify the samples into outliers varies with the data set used.\n",
    "\n",
    "<em>Doubt: </em>\n",
    "* We have adapted the implementation to our dataset. Hence, the results may not be generic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
